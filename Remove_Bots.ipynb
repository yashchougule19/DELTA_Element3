{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('datasets/btc_tweets_train.parquet.gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet ID</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>content</th>\n",
       "      <th>username</th>\n",
       "      <th>user_displayname</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1641579121972236290</td>\n",
       "      <td>[Bitcoin, Bitcoin, BTC, Bitcoin, BTC, SHIB, HO...</td>\n",
       "      <td>$Bitcoin TO $100,000 SOONER THAN YOU THINK‚ÄºÔ∏èüíØüôè...</td>\n",
       "      <td>BezosCrypto</td>\n",
       "      <td>SHIB Bezos</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1641579176171016194</td>\n",
       "      <td>[Bitcoin, bitcoinordinals, crypto]</td>\n",
       "      <td>Alright I have my rares. Who else is grabbing ...</td>\n",
       "      <td>spartantc81</td>\n",
       "      <td>SpartanTC</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1641579486071390208</td>\n",
       "      <td>[BTC, SHIB, HOGE, SAITAMA, BNB, DOGE, ETH, Bab...</td>\n",
       "      <td>Bitcoin (BTC) Targets Over $100,000 as This Im...</td>\n",
       "      <td>BezosCrypto</td>\n",
       "      <td>SHIB Bezos</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1641579537103302656</td>\n",
       "      <td>[BTC]</td>\n",
       "      <td>üì¢ Xverse Web-based pool is live:\\n\\n‚Ä¢Update @x...</td>\n",
       "      <td>godfred_xcuz</td>\n",
       "      <td>Algorithm.btc</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1641579588399804418</td>\n",
       "      <td>[Bitcoin]</td>\n",
       "      <td>Yesterday, a Bitcoin projection was displayed ...</td>\n",
       "      <td>goddess81oo</td>\n",
       "      <td>she is lucky</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet ID                                           hashtags  \\\n",
       "0  1641579121972236290  [Bitcoin, Bitcoin, BTC, Bitcoin, BTC, SHIB, HO...   \n",
       "1  1641579176171016194                 [Bitcoin, bitcoinordinals, crypto]   \n",
       "2  1641579486071390208  [BTC, SHIB, HOGE, SAITAMA, BNB, DOGE, ETH, Bab...   \n",
       "3  1641579537103302656                                              [BTC]   \n",
       "4  1641579588399804418                                          [Bitcoin]   \n",
       "\n",
       "                                             content      username  \\\n",
       "0  $Bitcoin TO $100,000 SOONER THAN YOU THINK‚ÄºÔ∏èüíØüôè...   BezosCrypto   \n",
       "1  Alright I have my rares. Who else is grabbing ...   spartantc81   \n",
       "2  Bitcoin (BTC) Targets Over $100,000 as This Im...   BezosCrypto   \n",
       "3  üì¢ Xverse Web-based pool is live:\\n\\n‚Ä¢Update @x...  godfred_xcuz   \n",
       "4  Yesterday, a Bitcoin projection was displayed ...   goddess81oo   \n",
       "\n",
       "  user_displayname  sentiment  \n",
       "0       SHIB Bezos       True  \n",
       "1        SpartanTC       True  \n",
       "2       SHIB Bezos       True  \n",
       "3    Algorithm.btc       True  \n",
       "4     she is lucky       True  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 6)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Username Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for usernames with a high number of digits\n",
    "df['num_digits_in_username'] = df['username'].apply(lambda x: sum(c.isdigit() for c in x))\n",
    "high_digit_usernames = df[df['num_digits_in_username'] > 5]  # Arbitrary threshold, can be adjusted\n",
    "\n",
    "# Check for repetitive characters in usernames\n",
    "df['repetitive_username'] = df['username'].apply(lambda x: any([x.count(char * 3) > 0 for char in set(x)]))\n",
    "repetitive_usernames = df[df['repetitive_username']]\n",
    "\n",
    "# Combine both checks\n",
    "suspicious_usernames = df[(df['num_digits_in_username'] > 5) | (df['repetitive_username'])]\n",
    "\n",
    "print(suspicious_usernames[['username', 'user_displayname', 'content']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Hashtag Analysis (This is still not generating satisfactory results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of hashtags in each tweet\n",
    "df['num_hashtags'] = df['content'].apply(lambda x: len(x.split()) if isinstance(x, str) else 0)\n",
    "\n",
    "# Flag tweets with more than a typical number of hashtags (e.g., >3)\n",
    "suspicious_hashtags = df[df['num_hashtags'] > 3]\n",
    "\n",
    "print(suspicious_hashtags[['username', 'num_hashtags', 'hashtags', 'content']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Content Similarity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet 0: $Bitcoin TO $100,000 SOONER THAN YOU THINK‚ÄºÔ∏èüíØüôè\n",
      "\n",
      "#Bitcoin¬†TO $100,000 WHETHER YOU BELIEVE OR NOT‚ÄºÔ∏èüíØüôè\n",
      "\n",
      "$BTC #Bitcoin¬†#BTC¬†¬†¬†\n",
      "\n",
      "#Bitcoin¬†#BTC¬†#SHIB \n",
      "#HOGE #SAITAMA #BNB¬†¬†¬†#DOGE #ETH #BabyFloki #AltCoinSeason https://t.co/rtlFlKlVCv\n",
      "Tweet 2: Bitcoin (BTC) Targets Over $100,000 as This Important Pattern Reemerges, Analyst Says\n",
      "\n",
      "$Bitcoin TO $100,000 SOONER THAN YOU THINK‚ÄºÔ∏èüíØüôè\n",
      "\n",
      "#BTC TO $100,000 WHETHER YOU BELIEVE OR NOT‚ÄºÔ∏èüíØüôè\n",
      "\n",
      "#SHIB \n",
      "#HOGE #SAITAMA #BNB¬†¬†¬†#DOGE #ETH #BabyFloki #AltCoinSeason  https://t.co/gU71C732NS\n",
      "-------\n",
      "Tweet 148: DigiByte makes everything easier! \n",
      "üì±üì±üì±üì±üì±üì±\n",
      "#Digibyte #DGB #Crypto #cryptocurrency #blockchain #payments #pay #Bitcoin¬†¬†¬†¬†#Bank #Digital $DGB https://t.co/Nty52pDB3I\n",
      "Tweet 196: ü™ôü™ôü™ôü™ôü™ôü™ô\n",
      "‚òëÔ∏èBanking is necessary‚ùóÔ∏è\n",
      "ü™ôü™ôü™ôü™ôü™ôü™ô\n",
      "‚òëÔ∏èBanks are not‚ùóÔ∏è\n",
      "ü™ôü™ôü™ôü™ôü™ôü™ô\n",
      "‚òëÔ∏è Be your own bank‚ÄºÔ∏è\n",
      "#Digibyte #DGB #Crypto #cryptocurrency #blockchain #payments #pay #Bitcoin¬†¬†¬†¬†¬†#Bank #Digital $DGB https://t.co/s12Z2fnx7H\n",
      "-------\n",
      "Tweet 233: Only at: #Shibariumbeta Transactions and wallet addresses skyrocketed at the last minute\n",
      "üëáüëá\n",
      "https://t.co/inT0cyY55c\n",
      "#Shibarium #SHIB #BONE #Bitcoin\n",
      "@Shibtoken @ShytoshiKusama @Shibariumtech https://t.co/9cUSQIDktB\n",
      "Tweet 523: Only at: #Shibariumbeta Transactions and wallet addresses skyrocketed at the last minute\n",
      "Follow me for updates..\n",
      "\n",
      "https://t.co/Vmk8G8AmPr\n",
      "#Shibarium #SHIB #BONE #Bitcoin¬†\n",
      "@Shibtoken @ShytoshiKusama @Shibariumtech. https://t.co/uRrRRBAkKw\n",
      "-------\n",
      "Tweet 244: I'm printing this on my T-Shirt.üëá\n",
      "$XEN #XENCrypto #XENFT #XENIANüßò‚Äç‚ôÄÔ∏è \n",
      ".....\n",
      "$ETH $MATIC $AVAX $DOT $BNB $EVMOS $GLMR $FTM $LUNC #DogeChain #OKXChain #Binance #Bitcoin\n",
      "Tweet 669: Bro describes $XEN #XENCrypto in \"Simple Words\" #XENFT #XENIANüßò‚Äç‚ôÄÔ∏è \n",
      ".....\n",
      "$ETH $MATIC $AVAX $DOT $BNB $EVMOS $GLMR $FTM $LUNC #DogeChain #OKXChain #Binance #Bitcoin https://t.co/GklhO1oIGG\n",
      "-------\n",
      "Tweet 300: $LUNC will skyrocket to during the next bull run.\n",
      "\n",
      "LIKE AND RETWEET IF YOU AGREE!\n",
      "\n",
      "#lunc #ustc #crypto #LUNCCcommunity #LuncArmy #Bitcoin #Binance https://t.co/pQHvx0UXX2\n",
      "Tweet 458: $LUNC will skyrocket during the next bull run.\n",
      "\n",
      "LIKE AND RETWEET IF YOU AGREE!\n",
      "#lunc #ustc #btc #usdt #Binance https://t.co/Ys3xHvs1oC\n",
      "-------\n",
      "Tweet 370: Stock futures are slightly higher early Friday - CNBC\n",
      "\n",
      "$SNAP $HOOD $CORZ $BKKT $AMC $NIO \n",
      "\n",
      "#BTC¬†¬†#SHIB \n",
      "#HOGE #SAITAMA #BNB¬†#DOGE #ETH #BabyFloki #AltCoinSeason \n",
      " https://t.co/UfSPNbPc3s\n",
      "Tweet 821: S&amp;P 500 futures rise as stocks wrap up a volatile, but winning first quarter - CNBC\n",
      "\n",
      "$SNAP $HOOD $CORZ $BKKT $AMC $NIO \n",
      "\n",
      "#BTC¬†¬†#SHIB \n",
      "#HOGE #SAITAMA #BNB¬†#DOGE #ETH #BabyFloki #AltCoinSeason \n",
      " https://t.co/UfSPNbPc3s\n",
      "-------\n",
      "Tweet 402: NO ACCOUNT SHOULD HAVE LESS THAN 1,000 FOLLOWERS üòé\n",
      "\n",
      "DROP AN EMOJI DOWN BELOW &amp; FOLLOW EVERYONE WHO LIKES IT ü§ù\n",
      "\n",
      "FOLLOW ME AND RETWEET SO IT FORMS A CHAIN REACTION ‚õìÔ∏è\n",
      "\n",
      "HAPPY FRIDAY GUYS, LETS GO üî•\n",
      "\n",
      "#crypto #bnb #YummyV2 #btc #bsc #Binance #cryptocurrency #nft #nfts #xrp #Floki https://t.co/DPDgpJgO15\n",
      "Tweet 1406: GLADIATORS ONLY!\n",
      "\n",
      "NO ACCOUNT SHOULD HAVE LESS THAN 1,200,000 FOLLOWERS üëÄ\n",
      "\n",
      "DROP AN EMOJI DOWN BELOW &amp; FOLLOW EVERYONE WHO LIKES IT \n",
      "\n",
      "RETWEET SO IT FORMS A CHAIN OF REACTION!\n",
      "\n",
      "LETS GO üî•\n",
      "\n",
      "#BNB #BTC #YUMMYV2 #CRYPTO #WEB3 #DEFI #BINANCE #ETH #NFT #NFTS #BNBCHAIN #XRP #YUMMYBONFIRE https://t.co/TxKMApwRZf\n",
      "-------\n",
      "Tweet 444: #Bitcoin approaches its 3rd consecutive green month, echoing its 2020-2021 price surge https://t.co/ydmHBZN8vJ\n",
      "Tweet 737: ICYMI: it seems like #Bitcoin is repeating its 2020-2021 price surge as the asset approaches its 3rd consecutive green month https://t.co/ydmHBZN8vJ #BTC $BTC\n",
      "-------\n",
      "Tweet 457: üöÄüåï Want to catch the next #Bitcoin? Look no further than #BorzoiInu! Join the #BorzArmy today and be part of the future of #cryptocurrency! üêïüíé #BorzArmy #altcoins #blockchain #HODL #1000x #Borz #GroveBlockchain #GRV #Shillme https://t.co/097lFJhTiG\n",
      "Tweet 1170: üëÄüåï Want to catch the next #Bitcoin? Look no further than #BorzoiInu! Join the #BorzArmy today and be part of the future of #cryptocurrencyüêïüíé #BorzArmy #altcoins #blockchain #hodl #1000x #Trending #Shillme #Grv #GroveBlockchain #Xrp #XRPArmy #Borz #HBAR #Doge #Shiba #SFM #Bnb https://t.co/1EHk9Yq4oy\n",
      "-------\n",
      "Tweet 529: Since 1st Jan 2023 returns:\n",
      "\n",
      "#Bitcoin is up by  72% üöÄ\n",
      "Gold: 7%\n",
      "Nasdaq: 15%\n",
      "\n",
      "#BTC #Crypto #cryptocurrency https://t.co/PZgvWJeWxJ\n",
      "Tweet 747: Since 1st Jan 2023 returns:\n",
      "\n",
      "#FLOKI¬† is up by  300% üöÄ\n",
      "Gold: 7%\n",
      "Nasdaq: 15%\n",
      "\n",
      "#BTC¬† #Crypto #cryptocurrency\n",
      "-------\n",
      "Tweet 558: If $LUNC‚ÄØ gets to $0.0001500 in next 24h, I will send $300 #lunc  to every person that like this tweet and follows! \n",
      "#xrp  #usdt¬† #btc¬† #Binance¬†#LuncBurn https://t.co/7S8WphKIlH\n",
      "Tweet 624: If $LUNC‚ÄØ gets to $0.0001400 in next 24h, I will send $300 #lunc  to 1  person that like this tweet and follows us\n",
      "#BabyDoge  #USDT #btc‚ÄØ #Binance https://t.co/EPcAXj980l\n",
      "-------\n",
      "Tweet 751: If you kick the hornets nest, the swarm will come towards you. #bitcoin https://t.co/noGy6fXpV8\n",
      "Tweet 920: @jimcramer If you kick the hornets nest, the swarm will come towards you. #bitcoin https://t.co/n1hhykRxpE\n",
      "-------\n",
      "Tweet 785: El Salvador has become the first country to adopt Bitcoin as legal tender. Will other countries follow ? #BTC  #Crypto\n",
      "Tweet 1227: Which country will be the next to adopt #Bitcoin‚ÄØ‚ÄØ‚ÄØ as legal tender? ü§î\n",
      "-------\n",
      "Tweet 1121: üö®BIG BREAKING- üá∫üá∏US Govt Sold 9,800 #BTC on March 14, Intends to Sell a Further 41,500 #BTC Connected to Silk Road in Four Batches Over the Course of the Year‚ÄºÔ∏èüò≤\n",
      "\n",
      "#Crypto #Bitcoin #cryptocurrency #cryptomarket #CryptoNews https://t.co/nmgcD6rp5a\n",
      "Tweet 1234: BREAKING üö®üö®\n",
      "\n",
      "US Govt Sold 9,800 #BTC on March 14 üò±\n",
      "\n",
      "It intends to sell further 41,490 #Bitcoin connected to Silk Road in four batches in 2023. https://t.co/zuaGAdWOiP\n",
      "-------\n",
      "Tweet 1133: üí•üá∫üá∏ US government sold 9,800 #Bitcoin¬†on March 14th.\n",
      "\n",
      "They intend to sell 41,500 more #BTC¬† connected to Silk Road over the next ~4 batches this year.\n",
      "Tweet 1155: The US Government sold 9,800 #bitcoin, seized during the Silk Road investigation, on the 14th March this year üí∞\n",
      "\n",
      "They intend on selling a further 41,500 bitcoin in four batches over the course of this year ü§î\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Vectorize the tweet content\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(df['content'])\n",
    "\n",
    "# Calculate cosine similarity matrix\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Set a threshold for similarity (e.g., >0.8) to find suspiciously similar tweets\n",
    "suspicious_pairs = []\n",
    "for i in range(len(cosine_sim)):\n",
    "    for j in range(i+1, len(cosine_sim)):\n",
    "        if cosine_sim[i, j] > 0.65:\n",
    "            suspicious_pairs.append((i, j))\n",
    "\n",
    "# Get the suspicious tweets based on content similarity\n",
    "for i, j in suspicious_pairs:\n",
    "    print(f\"Tweet {i}: {df.iloc[i]['content']}\")\n",
    "    print(f\"Tweet {j}: {df.iloc[j]['content']}\")\n",
    "    print(\"-------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skip the code in the above cell. Below is the same code as a function and will furter go in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def remove_similar_content(df, content_column, similarity_threshold=0.8):\n",
    "    \"\"\"\n",
    "    Removes samples with similar content from a DataFrame based on cosine similarity.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pd.DataFrame\n",
    "        The DataFrame containing the dataset.\n",
    "    - content_column: str\n",
    "        The name of the column containing the text content to analyze.\n",
    "    - similarity_threshold: float (default=0.8)\n",
    "        The threshold for cosine similarity above which samples are considered similar.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame\n",
    "        The DataFrame with similar content removed.\n",
    "    \"\"\"\n",
    "    # Vectorize the content column\n",
    "    tfidf = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = tfidf.fit_transform(df[content_column])\n",
    "\n",
    "    # Calculate cosine similarity matrix\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "    # Set to keep track of indices to remove\n",
    "    indices_to_remove = set()\n",
    "\n",
    "    # Loop through the similarity matrix to find similar content\n",
    "    for i in range(len(cosine_sim)):\n",
    "        for j in range(i + 1, len(cosine_sim)):\n",
    "            if cosine_sim[i, j] > similarity_threshold:\n",
    "                # Mark both indices for removal\n",
    "                indices_to_remove.add(i)\n",
    "                indices_to_remove.add(j)\n",
    "\n",
    "    # Create a new DataFrame excluding the indices found to be similar\n",
    "    df_cleaned = df.drop(index=indices_to_remove).reset_index(drop=True)\n",
    "\n",
    "    return df_cleaned\n",
    "\n",
    "# Example usage:\n",
    "# Assuming your data is in a DataFrame called `df` with a 'content' column\n",
    "# df_cleaned = remove_similar_content(df, content_column='content')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = remove_similar_content(df, 'content', 0.65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1408, 6)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "True     1134\n",
       "False     274\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Repetitive Phrases (N-gram Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detect n-grams that are overly common in tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk import ngrams\n",
    "import nltk\n",
    "\n",
    "# Make sure to download necessary NLTK data\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# Function to get n-grams\n",
    "def get_ngrams(text, n=3):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    return list(ngrams(words, n))\n",
    "\n",
    "# Apply n-gram extraction\n",
    "df['ngrams'] = df['content'].apply(lambda x: get_ngrams(x, n=5))\n",
    "\n",
    "# Flatten the list of n-grams and count frequencies\n",
    "ngram_list = [ngram for sublist in df['ngrams'] for ngram in sublist]\n",
    "ngram_freq = Counter(ngram_list)\n",
    "\n",
    "# Find common n-grams (e.g., those that appear more than 5 times)\n",
    "common_ngrams = {ngram: count for ngram, count in ngram_freq.items() if count > 5}\n",
    "print(common_ngrams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The methods for detecting and removing bot tweets should generally be applied **before** text processing (like removing stop words, lemmatization, punctuation removal, etc.). Here's why:\n",
    "\n",
    "### 1. **Bot Detection Is More Reliable on Raw Text**\n",
    "   - **Username and Content Similarity:** Bots often generate content that is templated or repetitive. If you process the text (e.g., removing stopwords or punctuation) before running similarity checks, you might lose some of the patterns that are indicative of bot activity. For example, removing punctuation could change the structure of the text, making similar bot-generated content appear more diverse than it actually is.\n",
    "   - **N-gram Analysis:** If you preprocess the text before generating n-grams, you might alter the n-grams in a way that makes them less effective for detecting repetitive phrases. For example, lemmatization could merge different forms of a word, potentially masking the repetitive nature of bot-generated text.\n",
    "\n",
    "### 2. **Cleaner Data for Sentiment Analysis**\n",
    "   - **After Bot Removal:** Once you've identified and removed potential bot tweets, you can then proceed with text processing like stop word removal, lemmatization, and punctuation removal. This ensures that the remaining text is cleaned and ready for more accurate sentiment analysis.\n",
    "\n",
    "### Workflow Suggestion:\n",
    "\n",
    "1. **Raw Data:**\n",
    "   - **Bot Detection:** Apply the username analysis, content similarity analysis, and possibly n-gram analysis on the raw text to flag and remove suspicious tweets.\n",
    "   \n",
    "2. **Text Processing:**\n",
    "   - **Cleaning:** After bot removal, proceed with standard text processing steps such as:\n",
    "     - Lowercasing\n",
    "     - Removing stop words\n",
    "     - Lemmatization or stemming\n",
    "     - Removing punctuation\n",
    "     - Tokenization\n",
    "\n",
    "3. **Sentiment Analysis:**\n",
    "   - **Model Training:** Use the cleaned dataset, now with reduced noise from potential bot-generated content, to train your sentiment analysis model.\n",
    "\n",
    "By following this approach, you ensure that bot detection is based on the full, unaltered text, making it more effective. After bot removal, your sentiment analysis can proceed with clean, human-generated content, leading to more reliable results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DELTA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
