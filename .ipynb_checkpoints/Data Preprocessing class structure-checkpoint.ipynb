{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure for Data pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "class DataPreprocessor:\n",
    "    def __init__(self, df, content_column, sentiment_column):\n",
    "        \"\"\"\n",
    "        Initialize the DataPreprocessor class with the DataFrame and relevant columns.\n",
    "\n",
    "        Parameters:\n",
    "        - df: pd.DataFrame\n",
    "            The DataFrame containing the dataset.\n",
    "        - content_column: str\n",
    "            The name of the column containing the text content to analyze.\n",
    "        - sentiment_column: str\n",
    "            The name of the column containing the sentiment labels.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.content_column = content_column\n",
    "        self.sentiment_column = sentiment_column\n",
    "    \n",
    "    def remove_similar_content(self, similarity_threshold=0.8):\n",
    "        \"\"\"\n",
    "        Removes samples with similar content based on cosine similarity.\n",
    "\n",
    "        Parameters:\n",
    "        - similarity_threshold: float (default=0.8)\n",
    "            The threshold for cosine similarity above which samples are considered similar.\n",
    "        \"\"\"\n",
    "        tfidf = TfidfVectorizer(stop_words='english')\n",
    "        tfidf_matrix = tfidf.fit_transform(self.df[self.content_column])\n",
    "        cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "        indices_to_remove = set()\n",
    "\n",
    "        for i in range(len(cosine_sim)):\n",
    "            for j in range(i + 1, len(cosine_sim)):\n",
    "                if cosine_sim[i, j] > similarity_threshold:\n",
    "                    indices_to_remove.add(i)\n",
    "                    indices_to_remove.add(j)\n",
    "\n",
    "        self.df = self.df.drop(index=indices_to_remove).reset_index(drop=True)\n",
    "\n",
    "    def handle_class_imbalance(self):\n",
    "        \"\"\"\n",
    "        Handles class imbalance using SMOTE (Synthetic Minority Over-sampling Technique).\n",
    "        \"\"\"\n",
    "        smote = SMOTE()\n",
    "        X = self.df[self.content_column]\n",
    "        y = self.df[self.sentiment_column]\n",
    "\n",
    "        X_resampled, y_resampled = smote.fit_resample(X.values.reshape(-1, 1), y)\n",
    "\n",
    "        self.df = pd.DataFrame({self.content_column: X_resampled.flatten(), self.sentiment_column: y_resampled})\n",
    "\n",
    "    def clean_text(self):\n",
    "        \"\"\"\n",
    "        Cleans text data by removing punctuation, stopwords, and applying lemmatization.\n",
    "        \"\"\"\n",
    "        nltk.download('stopwords')\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "\n",
    "        def clean(sentence):\n",
    "            # Remove punctuation and numbers\n",
    "            sentence = re.sub(r'[^a-zA-Z\\s]', '', sentence)\n",
    "            # Tokenize\n",
    "            words = nltk.word_tokenize(sentence)\n",
    "            # Remove stop words\n",
    "            words = [word for word in words if word.lower() not in stop_words]\n",
    "            # Lemmatize words\n",
    "            lemmatizer = nltk.WordNetLemmatizer()\n",
    "            words = [lemmatizer.lemmatize(word.lower()) for word in words]\n",
    "            return ' '.join(words)\n",
    "        \n",
    "        self.df[self.content_column] = self.df[self.content_column].apply(clean)\n",
    "\n",
    "    def preprocess(self, remove_similar=True, balance_classes=True, clean_text=True):\n",
    "        \"\"\"\n",
    "        Performs the full preprocessing pipeline.\n",
    "\n",
    "        Parameters:\n",
    "        - remove_similar: bool (default=True)\n",
    "            Whether to remove similar content.\n",
    "        - balance_classes: bool (default=True)\n",
    "            Whether to handle class imbalance.\n",
    "        - clean_text: bool (default=True)\n",
    "            Whether to clean text data.\n",
    "        \"\"\"\n",
    "        if remove_similar:\n",
    "            self.remove_similar_content()\n",
    "        if balance_classes:\n",
    "            self.handle_class_imbalance()\n",
    "        if clean_text:\n",
    "            self.clean_text()\n",
    "\n",
    "        return self.df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your DataFrame is `df`, with 'content' as the text column and 'sentiment' as the label column\n",
    "preprocessor = DataPreprocessor(df, content_column='content', sentiment_column='sentiment')\n",
    "\n",
    "# To run the full preprocessing pipeline:\n",
    "df_processed = preprocessor.preprocess()\n",
    "\n",
    "# If you only want to run specific steps, you can pass parameters:\n",
    "# For example, to remove similar content and clean text but skip class imbalance handling:\n",
    "df_processed = preprocessor.preprocess(balance_classes=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### COPIED CELL\n",
    "\n",
    "class DataPreprocessor:\n",
    "    def __init__(self, df, content_column, sentiment_column):\n",
    "        \"\"\"\n",
    "        Initialize the DataPreprocessor class with the DataFrame and relevant columns.\n",
    "\n",
    "        Parameters:\n",
    "        - df: pd.DataFrame\n",
    "            The DataFrame containing the dataset.\n",
    "        - content_column: str\n",
    "            The name of the column containing the text content to analyze.\n",
    "        - sentiment_column: str\n",
    "            The name of the column containing the sentiment labels.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.content_column = content_column\n",
    "        self.sentiment_column = sentiment_column\n",
    "    \n",
    "    def remove_spam_content(self, df, content_column, sentiment_column, similarity_threshold=0.65):\n",
    "        \"\"\"\n",
    "        Removes samples with similar content based on cosine similarity.\n",
    "\n",
    "        Parameters:\n",
    "        - similarity_threshold: float (default=0.65)\n",
    "            The threshold for cosine similarity above which samples are considered similar.\n",
    "        \"\"\"\n",
    "        tfidf = TfidfVectorizer(stop_words='english')\n",
    "        tfidf_matrix = tfidf.fit_transform(self.df[self.content_column])\n",
    "        cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "        indices_to_remove = set()\n",
    "\n",
    "        for i in range(len(cosine_sim)):\n",
    "            for j in range(i + 1, len(cosine_sim)):\n",
    "                if cosine_sim[i, j] > similarity_threshold:\n",
    "                    indices_to_remove.add(i)\n",
    "                    indices_to_remove.add(j)\n",
    "\n",
    "        self.df = self.df.drop(index=indices_to_remove).reset_index(drop=True)\n",
    "        return self.df\n",
    "    \n",
    "    def extract_hashtags(self, text):\n",
    "        \"\"\"Extract all hashtags from the text.\"\"\"\n",
    "        return re.findall(r'#\\w+', text)\n",
    "\n",
    "    def create_hashtag_whitelist(self, top_n=30):\n",
    "        \"\"\"Create a whitelist of the top N most frequent hashtags.\"\"\"\n",
    "        # Extract all hashtags from the dataset\n",
    "        all_hashtags = sum(self.df[self.content_column].apply(self.extract_hashtags), [])\n",
    "        # Count the occurrences of each hashtag\n",
    "        hashtag_counts = Counter(all_hashtags)\n",
    "        # Get the top N hashtags\n",
    "        top_hashtags = [hashtag for hashtag, count in hashtag_counts.most_common(top_n)]\n",
    "        return top_hashtags\n",
    "\n",
    "    def remove_hashtags(self, top_n=30):\n",
    "        \"\"\"Remove non-whitelisted hashtags from the text.\"\"\"\n",
    "        # Create the whitelist of top N hashtags\n",
    "        whitelist = self.create_hashtag_whitelist(top_n=top_n)\n",
    "        \n",
    "        def clean_hashtags(tweet):\n",
    "            hashtags = self.extract_hashtags(tweet)\n",
    "            # Retain only hashtags in the whitelist\n",
    "            for hashtag in hashtags:\n",
    "                if hashtag not in whitelist:\n",
    "                    tweet = tweet.replace(hashtag, '')\n",
    "            return tweet\n",
    "        \n",
    "        # Apply the cleaning function to the dataframe\n",
    "        self.df['hashtag_removed'] = self.df[self.content_column].apply(clean_hashtags)\n",
    "        return self.df    \n",
    "        \n",
    "    # def remove_hashtags(self):\n",
    "    #     \"\"\"\n",
    "    #     Removes hashtags from the text\n",
    "    #     \"\"\"\n",
    "    #     def clean_hashtags(tweet):\n",
    "    #         return re.sub(r'#\\w+', '', tweet)\n",
    "    \n",
    "    #     self.df['removed_hashtag'] = self.df[self.content_column].apply(clean_hashtags)\n",
    "    #     return self.df\n",
    "\n",
    "    def clean_text(self):\n",
    "        \"\"\"\n",
    "        Cleans text data by removing punctuation, stopwords, and applying lemmatization.\n",
    "        \"\"\"\n",
    "        nltk.download('stopwords')\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "\n",
    "        def clean(tweet):\n",
    "            # Convert emojis to text # Converts emojis to text, e.g., \"ðŸ˜Š\" becomes \":smiling_face:\"\n",
    "            tweet = emoji.demojize(tweet, delimiters=(\" \", \" \"))\n",
    "            # Remove links\n",
    "            tweet = re.sub(r'http\\S+|www\\S+|https\\S+', '', tweet, flags=re.MULTILINE)\n",
    "            # Remove punctuation and numbers\n",
    "            tweet = re.sub(r'[^a-zA-Z\\s]', '', tweet)\n",
    "            # Tokenize\n",
    "            words = nltk.word_tokenize(tweet)\n",
    "            # Remove stop words\n",
    "            words = [word for word in words if word.lower() not in stop_words]\n",
    "            # Lemmatize words\n",
    "            lemmatizer = nltk.WordNetLemmatizer()\n",
    "            words = [lemmatizer.lemmatize(word.lower()) for word in words]\n",
    "            return ' '.join(words)\n",
    "        \n",
    "        self.df['cleaned_content'] = self.df[self.content_column].apply(clean)\n",
    "        return self.df\n",
    "    \n",
    "    def handle_class_imbalance_with_SMOTE(self):\n",
    "        \"\"\"\n",
    "        Handles class imbalance using SMOTE (Synthetic Minority Over-sampling Technique).\n",
    "        \"\"\"\n",
    "        \n",
    "        tfidf = TfidfVectorizer(stop_words='english')\n",
    "        X = tfidf.fit_transform(self.df[self.content_column])\n",
    "        y = self.df[self.sentiment_column]\n",
    "        \n",
    "        # Apply SMOTE to the vectorized text\n",
    "        smote = SMOTE(random_state=21)\n",
    "        X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "        self.df = pd.DataFrame(X_resampled.toarray(), columns=tfidf.get_feature_names_out())\n",
    "        self.df[self.sentiment_column] = y_resampled\n",
    "        return self.df\n",
    "\n",
    "    def preprocess(self, remove_spam=False, remove_hashtags=False, clean_text=False, balance_classes=False):\n",
    "        \"\"\"\n",
    "        Performs the full preprocessing pipeline.\n",
    "\n",
    "        Parameters:\n",
    "        - remove_similar: bool (default=True)\n",
    "            Whether to remove similar content.\n",
    "        - balance_classes: bool (default=True)\n",
    "            Whether to handle class imbalance.\n",
    "        - clean_text: bool (default=True)\n",
    "            Whether to clean text data.\n",
    "        \"\"\"\n",
    "        if remove_spam:\n",
    "            self.remove_spam_content()\n",
    "        if remove_hashtags:\n",
    "            self.remove_hashtags()\n",
    "        if balance_classes:\n",
    "            self.handle_class_imbalance_with_SMOTE()\n",
    "        if clean_text:\n",
    "            self.clean_text()\n",
    "\n",
    "        return self.df\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
