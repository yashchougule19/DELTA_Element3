{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import emoji\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# For VaderSentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For RNN model \n",
    "from gensim.models import FastText\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, GRU, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_parquet('datasets/btc_tweets_train.parquet.gzip')\n",
    "test_df = pd.read_parquet('datasets/btc_tweets_test.parquet.gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.reset_index()\n",
    "test_df = test_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_info(df):\n",
    "    return df.shape, df.isnull().sum().sum(), df.info(), df.head()\n",
    "\n",
    "df_info(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop(['tweet ID', 'user_displayname', 'hashtags'], axis=1)\n",
    "test_df = test_df.drop(['tweet ID', 'user_displayname', 'hashtags'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the sentiment labels from bool to int\n",
    "train_df['sentiment'] = train_df['sentiment'].astype(int)\n",
    "test_df['sentiment'] = test_df['sentiment'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the DataPreprocessor class.\n",
    "        \n",
    "        Args commonly used in the Methods defined below:\n",
    "        \n",
    "        - df (pd.DataFrame): The DataFrame containing the dataset.\n",
    "        \n",
    "        - content_column (str): The name of the column containing the text content to analyze.\n",
    "        \n",
    "        - sentiment_column (str): The name of the column containing the sentiment labels.\n",
    "            \n",
    "        (other Args specific to defined methods are explained in that method's docstring)\n",
    "        \"\"\"\n",
    "    \n",
    "    def remove_spam_content(self, df, content_column, similarity_threshold=0.65):\n",
    "        \"\"\"\n",
    "        Removes extremly similar samples using cosine similarity.\n",
    "        \n",
    "        Args:\n",
    "        - similarity_threshold (float): (default=0.65) The threshold for cosine similarity above which samples are considered similar.\n",
    "        \"\"\"\n",
    "        tfidf = TfidfVectorizer(stop_words='english')\n",
    "        tfidf_matrix = tfidf.fit_transform(df[content_column])\n",
    "        cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "        indices_to_remove = set()\n",
    "\n",
    "        for i in range(len(cosine_sim)):\n",
    "            for j in range(i + 1, len(cosine_sim)):\n",
    "                if cosine_sim[i, j] > similarity_threshold:\n",
    "                    indices_to_remove.add(i)\n",
    "                    indices_to_remove.add(j)\n",
    "\n",
    "        df = df.drop(index=indices_to_remove).reset_index(drop=True)\n",
    "        return df\n",
    "\n",
    "    def remove_hashtags(self, df, content_column, top_n=30):\n",
    "        \"\"\"\n",
    "        Remove non-whitelisted hashtags from the tweet.\n",
    "        \n",
    "        Args:\n",
    "        - top_n (int): Retain top n number of hashtags and remove others from the tweets.\n",
    "        \"\"\"\n",
    "        \n",
    "        def extract_hashtags(tweet):\n",
    "            \"\"\"Extract all hashtags from the text.\"\"\"\n",
    "            return re.findall(r'#\\w+', tweet)\n",
    "        \n",
    "        def create_hashtag_whitelist(top_n):\n",
    "            \"\"\"Create a whitelist of the top N most frequent hashtags.\"\"\"\n",
    "            # Extract all hashtags from the dataset\n",
    "            all_hashtags = sum(df[content_column].apply(extract_hashtags), [])\n",
    "            # Count the occurrences of each hashtag\n",
    "            hashtag_counts = Counter(all_hashtags)\n",
    "            # Get the top N hashtags\n",
    "            top_hashtags = [hashtag for hashtag, count in hashtag_counts.most_common(top_n)]\n",
    "            return top_hashtags\n",
    "        \n",
    "        # Create the whitelist of top N hashtags\n",
    "        whitelist = create_hashtag_whitelist(top_n=top_n)\n",
    "        \n",
    "        def clean_hashtags(tweet):\n",
    "            hashtags = extract_hashtags(tweet)\n",
    "            # Retain only hashtags in the whitelist\n",
    "            for hashtag in hashtags:\n",
    "                if hashtag not in whitelist:\n",
    "                    tweet = tweet.replace(hashtag, '')\n",
    "            return tweet\n",
    "        \n",
    "        # Apply the cleaning function to the dataframe\n",
    "        df['hashtag_removed'] = df[content_column].apply(clean_hashtags)\n",
    "        return df\n",
    "    \n",
    "    def remove_link(self, df, content_column):\n",
    "        \"\"\"\n",
    "        Removes the links from the tweet\n",
    "        \"\"\"\n",
    "        def remove_links(tweet):\n",
    "            return re.sub(r'http\\S+|www\\S+|https\\S+', '', tweet, flags=re.MULTILINE)\n",
    "        df['links_removed'] = df[content_column].apply(remove_links)\n",
    "        return df\n",
    "    \n",
    "    def remove_whitespace_html(self, df, content_column):\n",
    "        \"\"\"\n",
    "        Removes unnecessary whitespace and html markups from the text\n",
    "        \"\"\"\n",
    "        def remove_whitesp(tweet):\n",
    "            tweet = BeautifulSoup(tweet, 'html.parser').get_text()\n",
    "            tweet = ' '.join(tweet.split())\n",
    "            return tweet\n",
    "        df['whitespace_html_removed'] = df[content_column].apply(remove_whitesp)\n",
    "        return df\n",
    "\n",
    "    def clean_text(self, df, content_column):\n",
    "        \"\"\"\n",
    "        Cleans text data by removing punctuation, stopwords, and applying lemmatization.\n",
    "        \"\"\"\n",
    "        nltk.download('stopwords')\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "\n",
    "        def clean(tweet):\n",
    "             # Remove links\n",
    "            tweet = re.sub(r'http\\S+|www\\S+|https\\S+', '', tweet, flags=re.MULTILINE)\n",
    "            # Convert emojis to text # Converts emojis to text, e.g., \"ğŸ˜Š\" becomes \":smiling_face:\"\n",
    "            tweet = emoji.demojize(tweet, delimiters=(\" \", \" \"))\n",
    "            # Removes unnecessary whitespace and html markups from the text\n",
    "            tweet = BeautifulSoup(tweet, 'html.parser').get_text()\n",
    "            tweet = ' '.join(tweet.split())\n",
    "            # Remove punctuation and numbers\n",
    "            tweet = re.sub(r'[^a-zA-Z\\s]', '', tweet)\n",
    "            # Tokenize\n",
    "            words = nltk.word_tokenize(tweet)\n",
    "            # Remove stop words\n",
    "            words = [word for word in words if word.lower() not in stop_words]\n",
    "            # Lemmatize words\n",
    "            lemmatizer = nltk.WordNetLemmatizer()\n",
    "            words = [lemmatizer.lemmatize(word.lower()) for word in words]\n",
    "            return ' '.join(words)\n",
    "        \n",
    "        df['cleaned_content'] = df[content_column].apply(clean)\n",
    "        return df\n",
    "    \n",
    "    def handle_class_imbalance_with_SMOTE(self):\n",
    "        \"\"\"\n",
    "        Handles class imbalance using SMOTE (Synthetic Minority Over-sampling Technique).\n",
    "        \"\"\"\n",
    "        \n",
    "        tfidf = TfidfVectorizer(stop_words='english')\n",
    "        X = tfidf.fit_transform(self.df[self.content_column])\n",
    "        y = self.df[self.sentiment_column]\n",
    "        \n",
    "        # Apply SMOTE to the vectorized text\n",
    "        smote = SMOTE(random_state=21)\n",
    "        X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "        self.df = pd.DataFrame(X_resampled.toarray(), columns=tfidf.get_feature_names_out())\n",
    "        self.df[self.sentiment_column] = y_resampled\n",
    "        return self.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapreprocessor = DataPreprocessor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: both the cleaned datasets above are still imbalanced with True values largly outnumbered than False. The imbalance needs to be taken care of by assigning class weights dring model training.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Benchmark: vaderSentiment Sentiment Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For sentiment analysis specifically using VADER, it's best to apply VADER to the raw, uncleaned text to leverage its strengths in handling informal language, punctuation, and emojis. However, when it comes to Links, it is best to remove them. Links are irrelevent to the sentiment and could add unnecessary noise, potentially influencing the sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessing flow for vaderSentiment\n",
    "- Remove spam tweets\n",
    "- Remove links\n",
    "- Remove unnecessary hashtags\n",
    "- Then apply vader sentiment dictionary on the cleaned tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Getting the dataset ready for VaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asign a \n",
    "vader_test_df = train_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove spam and bot samples\n",
    "vader_test_df = datapreprocessor.remove_spam_content(df=vader_test_df, content_column='content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove links from the tweets\n",
    "vader_test_df = datapreprocessor.remove_link(df=vader_test_df, content_column='content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary hashtags from the tweets\n",
    "vader_test_df = datapreprocessor.remove_hashtags(df=vader_test_df, content_column='links_removed', top_n = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_test_df = datapreprocessor.remove_whitespace_html(df=vader_test_df, content_column='hashtag_removed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look over on example to see if the links have been removed\n",
    "index = 4\n",
    "vader_test_df['content'].iloc[index], vader_test_df['links_removed'].iloc[index], vader_test_df['hashtag_removed'].iloc[index], vader_test_df['whitespace_html_removed'].iloc[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Fitting the VaderSentiment on processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the VADER sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to apply VADER sentiment analysis and get the compound score\n",
    "def get_sentiment_score(text):\n",
    "    sentiment_dict = analyzer.polarity_scores(text)\n",
    "    return sentiment_dict['compound']  # 'compound' score is a normalized score between -1 (negative) and +1 (positive)\n",
    "\n",
    "# Apply sentiment analysis to the 'content' column and create a new column for the sentiment score\n",
    "vader_test_df['vader_sentiment'] = vader_test_df['whitespace_html_removed'].apply(get_sentiment_score)\n",
    "\n",
    "# Optional: Classify the sentiment based on the compound score\n",
    "def classify_sentiment(score):\n",
    "    if score >= -0.03:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Apply the classification and create a new column for the sentiment label\n",
    "vader_test_df['vader_sentiment_label'] = vader_test_df['vader_sentiment'].apply(classify_sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vader_test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Evaluating the vaderSentiment classification performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = vader_test_df['sentiment']\n",
    "y_pred = vader_test_df['vader_sentiment_label']\n",
    "\n",
    "# Generate classification report\n",
    "print('Classification report:\\n')\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "# Generate a confusion matrix\n",
    "vader_conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(vader_conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative (0)', 'Positive(1)'], yticklabels=['Negative (0)', 'Positive (1)'])\n",
    "plt.ylabel('Actual Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.title('Confusion Matrx')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. RNN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Getting the dataset ready for generating the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove spam samples\n",
    "unspammed_train_df = datapreprocessor.remove_spam_content(df=train_df, content_column='content')\n",
    "unspammed_test_df = datapreprocessor.remove_spam_content(df=test_df, content_column='content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary hashtags\n",
    "unhashtag_train_df = datapreprocessor.remove_hashtags(df=unspammed_train_df, content_column='content')\n",
    "unhashtag_test_df = datapreprocessor.remove_hashtags(df=unspammed_test_df, content_column='content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Diya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "C:\\Users\\Diya\\AppData\\Local\\Temp\\ipykernel_37500\\945314507.py:109: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  tweet = BeautifulSoup(tweet, 'html.parser').get_text()\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Diya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "C:\\Users\\Diya\\AppData\\Local\\Temp\\ipykernel_37500\\945314507.py:109: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  tweet = BeautifulSoup(tweet, 'html.parser').get_text()\n"
     ]
    }
   ],
   "source": [
    "# Cleaned tweets (Free from links, emojis, whitespace, html markup and then Lemmetized)\n",
    "cleaned_train_df = datapreprocessor.clean_text(df=unhashtag_train_df, content_column='hashtag_removed')\n",
    "cleaned_test_df = datapreprocessor.clean_text(df=unhashtag_test_df, content_column='hashtag_removed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Training the FastText embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the cleaned tweets (split by spaces)\n",
    "tokenized_tweets = [tweet.split() for tweet in cleaned_train_df['cleaned_content']]\n",
    "\n",
    "# Train FastText model using Gensim's implementation\n",
    "fasttext_model = FastText(sentences=tokenized_tweets, vector_size=75, window=5, min_count=1, sg=1, epochs=10)\n",
    "\n",
    "# Save the model\n",
    "#fasttext_model.save(\"fasttext_vs50.model\")\n",
    "\n",
    "# Load the model (for future use)\n",
    "#fasttext_model = FastText.load(\"fasttext_vs50.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get vector for a word \n",
    "# print(f\"Vector for 'bitcoin': {fasttext_model.wv['bitcoin']}\")\n",
    "\n",
    "# Example: Get most similar words\n",
    "#print(f\"Words similar to 'bitcoin': {fasttext_model.wv.most_similar('bitcoin')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Building and Training the RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "max_sequence_length = 40\n",
    "embedding_dim = 75\n",
    "embedding_matrix = fasttext_model.wv.vectors # Create embedding matrix\n",
    "vocab_size = len(fasttext_model.wv) #len(tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fasttext_model.wv.vector_size, len(tokenizer.word_counts), vocab_size, tokenizer.word_index.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the tweets\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token='OOV')\n",
    "tokenizer.fit_on_texts(cleaned_train_df['cleaned_content'])\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(cleaned_train_df['cleaned_content']) # sequences are arrays where the each word is replaced by number which corresponds to the position of that word in the vocabulary\n",
    "padded_train_sequences = pad_sequences(train_sequences, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "test_sequences = tokenizer.texts_to_sequences(cleaned_test_df['cleaned_content'])\n",
    "padded_test_sequences = pad_sequences(test_sequences, maxlen=max_sequence_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1399, 40), (1399,))"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dimension check\n",
    "padded_train_sequences.shape, cleaned_train_df['sentiment'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data to training and validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(padded_train_sequences, cleaned_train_df['sentiment'], test_size = 0.2, random_state = 9)\n",
    "# Changing the dtype from series to array\n",
    "y_train = y_train.to_numpy()\n",
    "y_val = y_val.to_numpy()\n",
    "\n",
    "# Preparing the test data\n",
    "X_test = padded_test_sequences\n",
    "y_test = cleaned_test_df['sentiment'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 2.6023255813953488, 1: 0.6189159292035398}"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the class weights to handle imbalance\n",
    "class_weights = class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Study about kernel_regularizer, learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the RNN model\n",
    "def build_rnn_model(embd_dim, rnn_type='LSTM'):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(max_sequence_length,)))\n",
    "    model.add(Embedding(input_dim = vocab_size, \n",
    "                        output_dim = embd_dim, \n",
    "                        weights=[embedding_matrix],  \n",
    "                        trainable=False))\n",
    "    \n",
    "    if rnn_type == 'LSTM':\n",
    "        model.add(LSTM(units = 128, return_sequences = False))\n",
    "    elif rnn_type == 'GRU':\n",
    "        model.add(GRU(units = 128, return_sequences= False))\n",
    "        \n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation = 'sigmoid', kernel_regularizer=l2(0.02)))\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_20\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_20\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ embedding_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">75</span>)         â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">408,150</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">104,448</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ embedding_20 (\u001b[38;5;33mEmbedding\u001b[0m)        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m75\u001b[0m)         â”‚       \u001b[38;5;34m408,150\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_19 (\u001b[38;5;33mLSTM\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚       \u001b[38;5;34m104,448\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_19 (\u001b[38;5;33mDropout\u001b[0m)            â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_19 (\u001b[38;5;33mDense\u001b[0m)                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              â”‚           \u001b[38;5;34m129\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">512,727</span> (1.96 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m512,727\u001b[0m (1.96 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">104,577</span> (408.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m104,577\u001b[0m (408.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">408,150</span> (1.56 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m408,150\u001b[0m (1.56 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the model\n",
    "model = build_rnn_model(embd_dim=embedding_dim, rnn_type='LSTM')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.5922 - loss: 0.7407 - val_accuracy: 0.2179 - val_loss: 0.7346\n",
      "Epoch 2/15\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.4297 - loss: 0.7535 - val_accuracy: 0.7893 - val_loss: 0.7277\n",
      "Epoch 3/15\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.4637 - loss: 0.7595 - val_accuracy: 0.7893 - val_loss: 0.7223\n",
      "Epoch 4/15\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.5396 - loss: 0.7421 - val_accuracy: 0.7893 - val_loss: 0.7206\n",
      "Epoch 5/15\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.5661 - loss: 0.7253 - val_accuracy: 0.2107 - val_loss: 0.7330\n",
      "Epoch 6/15\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.3892 - loss: 0.7567 - val_accuracy: 0.7893 - val_loss: 0.7270\n",
      "Epoch 7/15\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.5073 - loss: 0.7384 - val_accuracy: 0.2107 - val_loss: 0.7342\n",
      "Epoch 8/15\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.5190 - loss: 0.7204 - val_accuracy: 0.7893 - val_loss: 0.7169\n",
      "Epoch 9/15\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.5392 - loss: 0.7127 - val_accuracy: 0.2107 - val_loss: 0.7357\n",
      "Epoch 10/15\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.4214 - loss: 0.7125 - val_accuracy: 0.2107 - val_loss: 0.7431\n",
      "Epoch 11/15\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.4332 - loss: 0.7346 - val_accuracy: 0.7893 - val_loss: 0.7225\n",
      "Epoch 12/15\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.5060 - loss: 0.7312 - val_accuracy: 0.7893 - val_loss: 0.7225\n",
      "Epoch 13/15\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.4696 - loss: 0.7418 - val_accuracy: 0.7893 - val_loss: 0.7225\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "history = model.fit(X_train, y_train, epochs=15, batch_size=32, validation_data=(X_val, y_val), class_weight=class_weights, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7923 - loss: 0.7167\n",
      "Test Loss: 0.7164530158042908\n",
      "Test Accuracy: 0.7978947162628174\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {loss}')\n",
    "print(f'Test Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n"
     ]
    }
   ],
   "source": [
    "# Predicting sentiments on new tweets\n",
    "predictions = model.predict(X_test)\n",
    "# Convert probabilities to binary predictions\n",
    "#predictions = (predictions > 0.612).astype(int)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n",
       "        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,\n",
       "        1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,\n",
       "        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,\n",
       "        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,\n",
       "        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1]),\n",
       " array([0.50937265, 0.50983953, 0.5094632 , 0.509395  , 0.5093719 ,\n",
       "        0.51045054, 0.50935555, 0.5095557 , 0.5093564 , 0.5097179 ,\n",
       "        0.5096629 , 0.50935996, 0.50962627, 0.509977  , 0.50936043,\n",
       "        0.50944924, 0.509362  , 0.5092791 , 0.50976074, 0.509362  ,\n",
       "        0.5093582 , 0.5093459 , 0.51026493, 0.50931937, 0.509362  ,\n",
       "        0.5093751 , 0.5092393 , 0.5093875 , 0.5095259 , 0.5093272 ,\n",
       "        0.5093895 , 0.5095763 , 0.5093317 , 0.5094    , 0.5093632 ,\n",
       "        0.51042974, 0.5095452 , 0.5093702 , 0.50946903, 0.5095431 ,\n",
       "        0.50992095, 0.5092891 , 0.5093541 , 0.51010376, 0.5093562 ,\n",
       "        0.5093631 , 0.50980085, 0.5093703 , 0.50953627, 0.5093776 ,\n",
       "        0.50936306, 0.5101347 , 0.5095092 , 0.5093794 , 0.50919425,\n",
       "        0.50933766, 0.509395  , 0.50964874, 0.5094235 , 0.5098088 ,\n",
       "        0.50940025, 0.50955415, 0.50949174, 0.50931764, 0.509396  ,\n",
       "        0.50932217, 0.5093713 , 0.50977105, 0.5093734 , 0.5093628 ,\n",
       "        0.5094975 , 0.5093605 , 0.5093541 , 0.50953746, 0.50997233,\n",
       "        0.5093612 , 0.5093703 , 0.5097082 , 0.5092945 , 0.5094328 ,\n",
       "        0.50973666, 0.50937295, 0.5097045 , 0.50935787, 0.510095  ,\n",
       "        0.50964046, 0.5092995 , 0.5101999 , 0.5096066 , 0.5098309 ,\n",
       "        0.509369  , 0.50947624, 0.50936747, 0.5097879 , 0.5094943 ,\n",
       "        0.5093782 , 0.5093734 , 0.50964355, 0.5093627 , 0.50937045,\n",
       "        0.5093591 , 0.50936466, 0.50971013, 0.509403  , 0.5093598 ,\n",
       "        0.5094191 , 0.5094348 , 0.5093529 , 0.50939643, 0.50951344,\n",
       "        0.5097372 , 0.5095537 , 0.5093827 , 0.50934744, 0.5094582 ,\n",
       "        0.5093584 , 0.50979924, 0.5094    , 0.5096967 , 0.5093582 ,\n",
       "        0.5095049 , 0.5093581 , 0.50946486, 0.50965166, 0.5093571 ,\n",
       "        0.50938976, 0.50936174, 0.50965446, 0.5094044 , 0.5093801 ,\n",
       "        0.5093661 , 0.5095085 , 0.50931   , 0.5095206 , 0.5098531 ,\n",
       "        0.5093746 , 0.50950295, 0.5093782 , 0.50943667, 0.5093718 ,\n",
       "        0.50933826, 0.50945485, 0.50968707, 0.5093609 , 0.5092593 ,\n",
       "        0.50954056, 0.5093688 , 0.50937283, 0.5102713 , 0.5093659 ,\n",
       "        0.5094179 , 0.5093424 , 0.5096547 , 0.5099636 , 0.5093671 ,\n",
       "        0.50934595, 0.5093751 , 0.5107394 , 0.50936466, 0.50934345,\n",
       "        0.50935847, 0.50938076, 0.50946045, 0.50920933, 0.5094249 ,\n",
       "        0.5093744 , 0.5093589 , 0.5093572 , 0.5095906 , 0.5093788 ,\n",
       "        0.5093533 , 0.5093335 , 0.5093599 , 0.50938666, 0.5093292 ,\n",
       "        0.50935376, 0.5096545 , 0.5099666 , 0.5093584 , 0.50935346,\n",
       "        0.5093632 , 0.50935936, 0.50935876, 0.5093437 , 0.50939614,\n",
       "        0.5093693 , 0.51169705, 0.5094301 , 0.50937855, 0.50952387,\n",
       "        0.509345  , 0.5100166 , 0.50973594, 0.5092557 , 0.509321  ,\n",
       "        0.50945884, 0.509475  , 0.50977457, 0.5097608 , 0.5098624 ,\n",
       "        0.50935477, 0.50978696, 0.5095991 , 0.51042604, 0.5093491 ,\n",
       "        0.50999236, 0.5093539 , 0.5095399 , 0.5094466 , 0.5102378 ,\n",
       "        0.5093512 , 0.50966656, 0.5095357 , 0.5101015 , 0.5105484 ,\n",
       "        0.5097345 , 0.5093852 , 0.5093589 , 0.5121262 , 0.5095407 ,\n",
       "        0.50935674, 0.5105517 , 0.5093429 , 0.50933754, 0.5093758 ,\n",
       "        0.5093601 , 0.51004684, 0.5093275 , 0.50970864, 0.5098375 ,\n",
       "        0.5096321 , 0.5097302 , 0.50946134, 0.5093832 , 0.50924397,\n",
       "        0.50940025, 0.509726  , 0.5093641 , 0.5094278 , 0.5093638 ,\n",
       "        0.50935096, 0.50938946, 0.5096024 , 0.5092637 , 0.5101925 ,\n",
       "        0.50943816, 0.5093628 , 0.50952405, 0.509485  , 0.5096359 ,\n",
       "        0.5099235 , 0.5101015 , 0.50936854, 0.5101369 , 0.50931644,\n",
       "        0.5094863 , 0.50938153, 0.510136  , 0.509813  , 0.5093698 ,\n",
       "        0.509362  , 0.5097167 , 0.5094261 , 0.509424  , 0.50935805,\n",
       "        0.50943756, 0.5094056 , 0.5093537 , 0.5093138 , 0.5093532 ,\n",
       "        0.5095525 , 0.50974584, 0.50936353, 0.5094003 , 0.5104337 ,\n",
       "        0.509354  , 0.5093496 , 0.50935495, 0.5093909 , 0.5095662 ,\n",
       "        0.50953203, 0.5097188 , 0.50935024, 0.5097772 , 0.5098196 ,\n",
       "        0.5097844 , 0.50936514, 0.50953853, 0.50936943, 0.5094081 ,\n",
       "        0.5099244 , 0.5094175 , 0.50948644, 0.509642  , 0.50999063,\n",
       "        0.5093812 , 0.5093622 , 0.5093734 , 0.5093425 , 0.5093608 ,\n",
       "        0.50978506, 0.509745  , 0.5093584 , 0.50939083, 0.5092336 ,\n",
       "        0.5099846 , 0.5095818 , 0.50969183, 0.5093347 , 0.5095417 ,\n",
       "        0.5095138 , 0.50942266, 0.50934315, 0.5093838 , 0.5094489 ,\n",
       "        0.51000726, 0.50936747, 0.5097003 , 0.50936496, 0.5093057 ,\n",
       "        0.50935864, 0.5100942 , 0.50968075, 0.50966287, 0.5093545 ,\n",
       "        0.5099227 , 0.50934654, 0.50936097, 0.50954145, 0.5093702 ,\n",
       "        0.5092697 , 0.50829256, 0.50932866, 0.50931966, 0.50939   ,\n",
       "        0.5093576 , 0.5107672 , 0.5093825 , 0.5093751 , 0.50937045,\n",
       "        0.51043   , 0.5093749 , 0.5093662 , 0.5095418 , 0.5093594 ,\n",
       "        0.5119126 , 0.50929224, 0.50957304, 0.50937265, 0.50936675,\n",
       "        0.5098083 , 0.5092843 , 0.5094075 , 0.51000535, 0.5098307 ,\n",
       "        0.50938654, 0.50939584, 0.5093942 , 0.50938284, 0.50969756,\n",
       "        0.5093558 , 0.509681  , 0.50935024, 0.50935364, 0.50946224,\n",
       "        0.5093536 , 0.50952125, 0.5094152 , 0.5093739 , 0.50945425,\n",
       "        0.5093499 , 0.50941503, 0.5095885 , 0.5093744 , 0.5093595 ,\n",
       "        0.5093833 , 0.50936306, 0.5093696 , 0.5094893 , 0.5094652 ,\n",
       "        0.5094644 , 0.50940096, 0.50937736, 0.50979817, 0.51000255,\n",
       "        0.50937915, 0.5094741 , 0.5093728 , 0.5093956 , 0.5093428 ,\n",
       "        0.5095423 , 0.5094974 , 0.5096611 , 0.5094328 , 0.50945187,\n",
       "        0.50936043, 0.5094037 , 0.5100099 , 0.50941026, 0.50933886,\n",
       "        0.50936604, 0.509375  , 0.50931334, 0.5096806 , 0.5093733 ,\n",
       "        0.50936115, 0.50947285, 0.5103477 , 0.50942993, 0.50969917,\n",
       "        0.5094583 , 0.5102227 , 0.5095283 , 0.5093818 , 0.50933856,\n",
       "        0.5093602 , 0.50936055, 0.5095866 , 0.50942445, 0.50936437,\n",
       "        0.5093472 , 0.5093632 , 0.50946766, 0.5093457 , 0.50936073,\n",
       "        0.5097625 , 0.50950783, 0.50950193, 0.5095542 , 0.51031405,\n",
       "        0.5095088 , 0.5094435 , 0.5092758 , 0.50934935, 0.50934243,\n",
       "        0.5093672 , 0.51039904, 0.50993013, 0.51004905, 0.50951076,\n",
       "        0.5093448 , 0.50936866, 0.509361  , 0.5093622 , 0.5095905 ,\n",
       "        0.50936276, 0.5095587 , 0.509359  , 0.5101805 , 0.5094069 ,\n",
       "        0.5093875 , 0.5096082 , 0.5093607 , 0.51022613, 0.50935984,\n",
       "        0.5094089 , 0.50943834, 0.5095459 , 0.5102576 , 0.5095291 ,\n",
       "        0.5103315 , 0.50935984, 0.50935066, 0.5093484 , 0.50934666,\n",
       "        0.50934315, 0.50936145, 0.5093625 , 0.50902545, 0.5095997 ,\n",
       "        0.5093187 , 0.5093548 , 0.51008075, 0.50940883, 0.5095179 ],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test, predictions.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0049908757"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.max()-predictions.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      0.11      0.16        96\n",
      "           1       0.80      0.92      0.86       379\n",
      "\n",
      "    accuracy                           0.76       475\n",
      "   macro avg       0.54      0.52      0.51       475\n",
      "weighted avg       0.70      0.76      0.72       475\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 11,  85],\n",
       "       [ 30, 349]], dtype=int64)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(classification_report(y_test, predictions))\n",
    "confusion_matrix(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "1    379\n",
       "0     96\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_test_df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_roe = [index for index, row in enumerate(X_val) if len(row) > 50]\n",
    "\n",
    "print(invalid_roe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "tokenizer = Tokenizer(num_words=5000, oov_token=1)\n",
    "# The oov_token is a placeholder token that replaces any OOV words during the text_to_sequence calls.\n",
    "#This ensures that your model can handle new, unseen words gracefully.\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On how many tweets did we train?\n",
    "print(tokenizer.document_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many unique words?\n",
    "len(tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_val_seq = tokenizer.texts_to_sequences(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the sequences\n",
    "max_length = 28  # Set max length for padding\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_length, padding='post')\n",
    "X_val_pad = pad_sequences(X_val_seq, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create an embedding matrix\n",
    "embedding_dim = 100  # Must match the dimension of FastText vectors\n",
    "vocab_size = len(tokenizer.word_index) + 1   #it's the total number of unique words in your tokenizerâ€™s vocabulary plus one. The +1 accounts for the padding token (index 0).\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim)) #embedding_matrix is initialized as a matrix of zeros with shape (vocab_size, embedding_dim). \n",
    "                                                        #This matrix will eventually hold the FastText vectors for each word in your vocabulary.\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in fasttext_model.wv:\n",
    "        embedding_matrix[i] = fasttext_model.wv[word]\n",
    "\n",
    "print(f\"Embedding matrix shape: {embedding_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import keras as kp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, \n",
    "                    output_dim=embedding_dim, \n",
    "                    weights=[embedding_matrix], \n",
    "                    input_length=max_length, \n",
    "                    trainable=False))\n",
    "model.add(LSTM(128, return_sequences=False))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming 'contents' is the column with the tweets and 'sentiment' is the label (0 or 1)\n",
    "texts = cleaned_train_df['cleaned_contents'].astype(str).tolist() \n",
    "labels = cleaned_train_df['sentiment'].values\n",
    "\n",
    "# Load pre-trained FastText embeddings\n",
    "fasttext.util.download_model('en', if_exists='ignore')  # Download the model if not already present\n",
    "ft = fasttext.load_model('cc.en.300.bin')  # Load the pre-trained FastText model\n",
    "\n",
    "# Tokenize the texts\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# Vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Pad sequences to ensure uniform input length\n",
    "max_sequence_length = 50  # You can adjust this based on your data\n",
    "X = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Create an embedding matrix using the FastText embeddings\n",
    "embedding_dim = 300  # FastText embeddings typically have 300 dimensions\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i < vocab_size:\n",
    "        embedding_vector = ft.get_word_vector(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Compute class weights to handle imbalance\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# Build the RNN model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, \n",
    "                    output_dim=embedding_dim, \n",
    "                    weights=[embedding_matrix], \n",
    "                    input_length=max_sequence_length, \n",
    "                    trainable=False))  # Set trainable to False to keep FastText embeddings static\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, \n",
    "          epochs=10, \n",
    "          batch_size=32, \n",
    "          validation_data=(X_val, y_val), \n",
    "          class_weight=class_weights_dict)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_val, y_val)\n",
    "print(f'Validation Loss: {loss}, Validation Accuracy: {accuracy}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DELTA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
