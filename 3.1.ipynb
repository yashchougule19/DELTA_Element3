{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d60fb37-da97-4212-a855-8a3c613a37d5",
   "metadata": {},
   "source": [
    "## Importing and getting to know the Df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45b0a4cd-e589-4ca9-ac8f-2b0fa87af2d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Diya\\Documents\\GItHub\\DELTA_Element3\n"
     ]
    }
   ],
   "source": [
    "import os  # operating system interface \n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fad7f7be-f743-4320-bd68-6dcb6ccf9d5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "#with gzip.open('btc_tweets_train.parquet.gzip', 'rt') as f:\n",
    " #   content = f.read()\n",
    "    # Process the content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd3f8b4c-adcc-4024-a474-e08ad7bcc90e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>content</th>\n",
       "      <th>username</th>\n",
       "      <th>user_displayname</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweet ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1641579121972236290</th>\n",
       "      <td>[Bitcoin, Bitcoin, BTC, Bitcoin, BTC, SHIB, HO...</td>\n",
       "      <td>$Bitcoin TO $100,000 SOONER THAN YOU THINK‚ÄºÔ∏èüíØüôè...</td>\n",
       "      <td>BezosCrypto</td>\n",
       "      <td>SHIB Bezos</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1641579176171016194</th>\n",
       "      <td>[Bitcoin, bitcoinordinals, crypto]</td>\n",
       "      <td>Alright I have my rares. Who else is grabbing ...</td>\n",
       "      <td>spartantc81</td>\n",
       "      <td>SpartanTC</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1641579486071390208</th>\n",
       "      <td>[BTC, SHIB, HOGE, SAITAMA, BNB, DOGE, ETH, Bab...</td>\n",
       "      <td>Bitcoin (BTC) Targets Over $100,000 as This Im...</td>\n",
       "      <td>BezosCrypto</td>\n",
       "      <td>SHIB Bezos</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1641579537103302656</th>\n",
       "      <td>[BTC]</td>\n",
       "      <td>üì¢ Xverse Web-based pool is live:\\n\\n‚Ä¢Update @x...</td>\n",
       "      <td>godfred_xcuz</td>\n",
       "      <td>Algorithm.btc</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1641579588399804418</th>\n",
       "      <td>[Bitcoin]</td>\n",
       "      <td>Yesterday, a Bitcoin projection was displayed ...</td>\n",
       "      <td>goddess81oo</td>\n",
       "      <td>she is lucky</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                              hashtags  \\\n",
       "tweet ID                                                                 \n",
       "1641579121972236290  [Bitcoin, Bitcoin, BTC, Bitcoin, BTC, SHIB, HO...   \n",
       "1641579176171016194                 [Bitcoin, bitcoinordinals, crypto]   \n",
       "1641579486071390208  [BTC, SHIB, HOGE, SAITAMA, BNB, DOGE, ETH, Bab...   \n",
       "1641579537103302656                                              [BTC]   \n",
       "1641579588399804418                                          [Bitcoin]   \n",
       "\n",
       "                                                               content  \\\n",
       "tweet ID                                                                 \n",
       "1641579121972236290  $Bitcoin TO $100,000 SOONER THAN YOU THINK‚ÄºÔ∏èüíØüôè...   \n",
       "1641579176171016194  Alright I have my rares. Who else is grabbing ...   \n",
       "1641579486071390208  Bitcoin (BTC) Targets Over $100,000 as This Im...   \n",
       "1641579537103302656  üì¢ Xverse Web-based pool is live:\\n\\n‚Ä¢Update @x...   \n",
       "1641579588399804418  Yesterday, a Bitcoin projection was displayed ...   \n",
       "\n",
       "                         username user_displayname  sentiment  \n",
       "tweet ID                                                       \n",
       "1641579121972236290   BezosCrypto       SHIB Bezos       True  \n",
       "1641579176171016194   spartantc81        SpartanTC       True  \n",
       "1641579486071390208   BezosCrypto       SHIB Bezos       True  \n",
       "1641579537103302656  godfred_xcuz    Algorithm.btc       True  \n",
       "1641579588399804418   goddess81oo     she is lucky       True  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet('datasets/btc_tweets_train.parquet.gzip', engine='pyarrow')\n",
    "# Display the first few rows of the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596d6aa4-38d6-43fb-ad73-ff37790283f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7722e40-6f00-44e6-bbcd-a89385f98d5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355645ef-3d09-4467-aad6-b44fceff1a70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.iloc[1299,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d7a122-e540-40be-8425-87ba7b0e2417",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3b845f-9228-4aa5-a611-19faea4c0e92",
   "metadata": {},
   "source": [
    "## Df Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfb6fae-acb9-4aaf-8d1b-10ce499d2819",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.reset_index(inplace=True)\n",
    "df.drop(columns=['tweet ID','hashtags','username','user_displayname'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f4b968-70a9-47d8-91e8-13ad8de4a8cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Get the number of duplicated rows and remove any duplicates after that:\n",
    "print('Number of duplicated samples: ',df[df.duplicated()].shape[0])\n",
    "df=df.drop_duplicates(keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7c08d9-8042-4655-ab84-a14acef52a93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429f51b4-8ff0-4f59-8fa3-71822256b534",
   "metadata": {},
   "source": [
    "## Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa241894-cc5e-456a-be94-820efa151efb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_whitespace(text):\n",
    "    \"\"\" Function to remove whitespace (tabs, newlines). \"\"\"\n",
    "    return ' '.join(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcd7fff-2894-4a7e-9da2-b02bc0d1d0d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_links(text):\n",
    "    \"\"\"Function to remove hyperlinks from the text.\"\"\"\n",
    "    return re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2509cb2-5c0a-4adc-b394-2fbb8ac8cbc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_punctuation_and_casing(text):\n",
    "    \"\"\"\n",
    "    Function to remove the punctuation, upper casing and words that include\n",
    "    non-alphanumeric characters.\n",
    "    \"\"\"\n",
    "    chars = '!\\\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~'\n",
    "    text = text.translate(str.maketrans(chars, ' ' * len(chars)))\n",
    "    return ' '.join([word.lower() for word in text.split() if word.isalpha()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a35761-eefd-4c47-ac1c-983d83f76e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "english_stopwords = stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \"\"\" Function to remove stopwords. \"\"\"\n",
    "    return ' '.join([word for word in str(text).split() if not word in english_stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777d59bd-9850-4527-a5a6-6739fcac58c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_words(text, **kwargs):\n",
    "    \"\"\" Function to lemmatize words. \"\"\"\n",
    "    return ' '.join([lemmatizer.lemmatize(word, **kwargs) for word in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796552a8-2ee5-4f77-8535-77d571ad0e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209fed9a-8cc7-48f6-a886-5db01c80cb19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Function to preprocess text by removing links, whitespace, punctuation, and converting to lower case.\"\"\"\n",
    "    text = remove_links(text)\n",
    "    text = remove_whitespace(text)\n",
    "    text = remove_punctuation_and_casing(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = lemmatize_words(text)\n",
    "    text =  ' '.join(word_tokenize(text)) #to return only the text and not a list\n",
    "#    text = stem_words(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665d22be-0c7d-492c-bb0b-dbef58b8f2a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['cleaned_content'] = df['content'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f70e5e-a505-48e9-b8bf-c2e33b4a5d4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8372bbd5-33ef-4194-878d-1e8c0b7b13ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## After the process:\n",
    "# Check all is well\n",
    "ix = 12  # just one example, play with other play to further examine the effect of our clearning\n",
    "print('Original Review:\\n' + df.content[ix])  \n",
    "print('\\nCleaned Review:\\n' + df.cleaned_content[ix])\n",
    "#from nlp_foundations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d8c0fb-6473-4b7e-acec-ff53b8898d9a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## (1) Sentiment Dictionary Benchmark - vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306616a2-e03b-40a7-8fd7-7eafe1f658d1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec1835f-d992-44b3-81c2-0135129b560e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import re\n",
    "\n",
    "def remove_links(text):\n",
    "    return re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "def remove_whitespace(text):\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "def remove_punctuation_and_casing(text):\n",
    "    chars = '!\\\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~'\n",
    "    text = text.translate(str.maketrans(chars, ' ' * len(chars)))\n",
    "    return ' '.join([word.lower() for word in text.split() if word.isalpha()])\n",
    "\n",
    "def preprocess_tweet(text):\n",
    "    text = remove_links(text)\n",
    "    text = remove_whitespace(text)\n",
    "    text = remove_punctuation_and_casing(text)\n",
    "    return text\n",
    "\n",
    "# Example tweets\n",
    "tweets = [\"I love this product! #awesome\", \"This is the worst experience ever! http://example.com\"]\n",
    "\n",
    "# Preprocess tweets\n",
    "preprocessed_tweets = [preprocess_tweet(tweet) for tweet in tweets]\n",
    "\n",
    "# Initialize VADER sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Analyze sentiment\n",
    "for tweet in preprocessed_tweets:\n",
    "    sentiment = analyzer.polarity_scores(tweet)\n",
    "    print(f\"Tweet: {tweet}\")\n",
    "    print(f\"Sentiment: {sentiment}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def37ee6-9d06-4784-8252-695d784f172c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#dictionary = pd.DataFrame(df['cleaned_content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae2d6b2-fe99-4778-a149-ba9e9026b3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    sentiment_dict = analyzer.polarity_scores(text)\n",
    "    return sentiment_dict\n",
    "\n",
    "# Apply sentiment analysis to each tweet\n",
    "df['vader_sentiment'] = df['cleaned_content'].apply(analyze_sentiment)\n",
    "\n",
    "# Extract compound score for simplicity\n",
    "df['vader_compound'] = df['vader_sentiment'].apply(lambda x: x['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0e2a07-0334-4961-bba2-8f1a331656c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(df['vader_compound'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d43cb0-e6b5-49be-811a-dfd24f2a8166",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "print(df['vader_sentiment'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a584f6-b617-418d-864c-0b4e63938718",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert compound score to sentiment label\n",
    "def compound_to_binary_sentiment(compound_score):\n",
    "    if compound_score > 0:\n",
    "        return 1  # Positive\n",
    "    elif compound_score <= 0:\n",
    "        return 0  # Negative\n",
    "\n",
    "df['vader_label'] = df['vader_compound'].apply(compound_to_binary_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f51ba41-c226-4f0a-98f8-5243549fabad",
   "metadata": {},
   "source": [
    "### Sentiment classifier assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124b129d-0a5d-4f42-9454-23645d1a946e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Comparison (If you have actual labels, you can evaluate the performance)\n",
    "if 'sentiment' in df.columns:\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(df['sentiment'], df['vader_label'])\n",
    "    precision = precision_score(df['sentiment'], df['vader_label'], average='weighted')\n",
    "    recall = recall_score(df['sentiment'], df['vader_label'], average='weighted')\n",
    "    f1 = f1_score(df['sentiment'], df['vader_label'], average='weighted')\n",
    "\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1-score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7c71d2-86bc-48f1-b239-b23a51ea78b1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1f8330-beaa-43a8-bad8-3d8cf938eb8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['vader_compound'].hist(); \n",
    "df['vader_compound'].describe() # overall rather positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51158de-7e80-42d8-b77e-eceb9e65e0f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#check #https://github.com/Humboldt-WI/delta/blob/master/demos/nlp/sentiment_analysis.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4720f1-b99b-4b08-854b-63eb43d66ba1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Check dropping neutral compound values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a012bc5-4103-49ea-a0a6-4551af2483fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compound_to_binary_sentiment_2(compound_score):\n",
    "    if compound_score >= 0.05:\n",
    "        return 1  # Positive\n",
    "    elif compound_score <= -0.05:\n",
    "        return 0  # Negative\n",
    "    else:\n",
    "        return None  # Neutral, will be ignored\n",
    "\n",
    "df['vader_binary_label_2'] = df['vader_compound'].apply(compound_to_binary_sentiment_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c7128d-cded-44f4-b632-9cb1674306ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "temp = df.copy()\n",
    "temp = temp.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e782d7-94b9-4db1-8593-447fc59fbfa2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "temp['vader_binary_label_2'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadbb265-b9dc-420d-8ff5-25fe246ba186",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert boolean values to integers\n",
    "temp['sentiment'] = temp['sentiment'].astype(int)\n",
    "\n",
    "# Print the first few rows to verify the change\n",
    "print(temp.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc59bd0a-30c7-4eae-a368-bbb19b11268c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Comparison (If you have actual labels, you can evaluate the performance)\n",
    "if 'sentiment' in temp.columns:\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(temp['sentiment'], temp['vader_binary_label_2'])\n",
    "    precision = precision_score(temp['sentiment'], temp['vader_binary_label_2'], average='weighted')\n",
    "    recall = recall_score(temp['sentiment'], temp['vader_binary_label_2'], average='weighted')\n",
    "    f1 = f1_score(temp['sentiment'], temp['vader_binary_label_2'], average='weighted')\n",
    "\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1-score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde02217-a5e5-4da2-8f37-6b0627b517cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print the DataFrame to see the results\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcdb899-bc32-44b9-9b58-75b036b6cf73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "354fc1e0-ce15-4ad2-b034-53ad126dcacf",
   "metadata": {},
   "source": [
    "## Embedding - FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1746f3-0b15-489e-b9e2-ed6371fda554",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# Download and load the sample dataset\n",
    "nltk.download('brown')\n",
    "sentences = brown.sents()\n",
    "\n",
    "# Train the Word2Vec model\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=2, workers=4)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"word2vec.model\")\n",
    "\n",
    "# Load the model\n",
    "model = Word2Vec.load(\"word2vec.model\")\n",
    "\n",
    "# Get the vector for a word\n",
    "vector = model.wv['king']\n",
    "print(vector)\n",
    "\n",
    "# Find most similar words\n",
    "similar_words = model.wv.most_similar('king')\n",
    "print(similar_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be56b129-e227-41b5-b061-72fcf5a8fca4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gensim.models import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1065ac85-ede3-464d-a004-804d7e218373",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "type(df['cleaned_content'].iloc[1,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec03fcf-daba-42d9-97a4-0f7d2adfb049",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['cleaned_content'].iloc[1,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddf1337-5419-49a2-b0fb-b0a8636e2260",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenize the cleaned tweets (split by spaces)\n",
    "tokenized_tweets = [tweet.split() for tweet in df['cleaned_content']]\n",
    "\n",
    "# Train FastText model using Gensim's implementation\n",
    "fasttext_model = FastText(sentences=tokenized_tweets, vector_size=100, window=5, min_count=3, sg=1, epochs=10)\n",
    "\n",
    "# Save the model\n",
    "fasttext_model.save(\"fasttext.model\")\n",
    "\n",
    "# Load the model (for future use)\n",
    "fasttext_model = FastText.load(\"fasttext.model\")\n",
    "\n",
    "# Example: Get vector for a word \n",
    "print(f\"Vector for 'bitcoin': {fasttext_model.wv['bitcoin']}\")\n",
    "\n",
    "# Example: Get most similar words\n",
    "print(f\"Words similar to 'bitcoin': {fasttext_model.wv.most_similar('bitcoin')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3a5839-8c99-4cb4-b058-f16e47666d32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c808ef-dc02-478a-ac9d-3f58d9403f66",
   "metadata": {},
   "source": [
    "## (2) RNN-based language classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc65ff66-428c-4ee2-bced-58ff921369ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import InputLayer, LSTM, Dense, Dropout, SimpleRNN\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa8eac9-99b4-4406-8216-c4f43a281b80",
   "metadata": {},
   "source": [
    "Notes: \n",
    "- So using two embedding layers is important.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203333f1-63be-4046-bed6-ff6905f8c1cf",
   "metadata": {},
   "source": [
    "### Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232dc3ad-44dd-488d-b0ab-e88c99b39530",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(df['cleaned_content'],df['sentiment'], test_size = 0.2, random_state = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734ecfbb-c903-485e-a075-fff692121664",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train.shape, X_val.shape, y_train.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d082b5-3e22-4a47-8baf-3584909fbcaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a673aa6e-dda3-42df-878f-98d67f5d1a19",
   "metadata": {},
   "source": [
    "### Building vocabulary - Tokenizing & Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9629b30a-087c-42a6-85b1-e52fec07b4b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede4519a-9569-421f-9fbf-ddbd7a10c0f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "tokenizer = Tokenizer(num_words=5000, oov_token=1)\n",
    "# The oov_token is a placeholder token that replaces any OOV words during the text_to_sequence calls.\n",
    "#This ensures that your model can handle new, unseen words gracefully.\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7fc973-9efb-40c4-9446-762f769eaaa0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# On how many tweets did we train?\n",
    "print(tokenizer.document_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018ef035-83e7-4cfd-97a2-03b3205c29db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# How many unique words?\n",
    "len(tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7a39c1-0558-48fb-8b5c-a674abb9f126",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A dictionary of words storing in how many documents a word appeared\n",
    "word = 'bitcoin'  \n",
    "n = tokenizer.word_docs[word]\n",
    "print('The word <{}> appeared in {} tweets.'.format(word, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66072337-4eb8-4335-aac7-837798e986b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_val_seq = tokenizer.texts_to_sequences(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf17446-3dde-4b52-aa85-bcda124203dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Calculate the number of tokens per document\n",
    "num_tokens_per_doc = [len(seq) for seq in X_train_seq]\n",
    "\n",
    "# Plot the distribution\n",
    "plt.hist(num_tokens_per_doc, bins=50)\n",
    "plt.xlabel('Number of Tokens')\n",
    "plt.ylabel('Number of Documents')\n",
    "plt.title('Distribution of Tokens per Document')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e29cda-92ba-43f0-99f8-4755c97cd60b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calculate the 95th percentile of the number of tokens per document\n",
    "max_length = int(np.percentile(num_tokens_per_doc, 95))\n",
    "print(f\"Recommended max length for padding: {max_length}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4c2d0a-9701-439f-968d-c3f50e1ac188",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pad the sequences\n",
    "max_length = 28  # Set max length for padding\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_length, padding='post')\n",
    "X_val_pad = pad_sequences(X_val_seq, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571d6bfb-c5d4-4c3f-8250-f9b99c1d45bc",
   "metadata": {},
   "source": [
    "### Load own (FastText) Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa420fd-39bd-4830-8001-f03250f679ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create an embedding matrix\n",
    "embedding_dim = 100  # Must match the dimension of FastText vectors\n",
    "vocab_size = len(tokenizer.word_index) + 1   #it's the total number of unique words in your tokenizer‚Äôs vocabulary plus one. The +1 accounts for the padding token (index 0).\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim)) #embedding_matrix is initialized as a matrix of zeros with shape (vocab_size, embedding_dim). \n",
    "                                                        #This matrix will eventually hold the FastText vectors for each word in your vocabulary.\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in fasttext_model.wv:\n",
    "        embedding_matrix[i] = fasttext_model.wv[word]\n",
    "\n",
    "print(f\"Embedding matrix shape: {embedding_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89610a60-20a7-4b5b-8a6f-f2ad5ae5e026",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff8e25b-4072-439d-9898-308b2fd53654",
   "metadata": {},
   "source": [
    "### Simple RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d9b542-4554-4685-bca8-d369c59fc6cc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model definition\n",
    "SR_model = Sequential() # Input shape\n",
    "SR_model.add(SimpleRNN(100, activation='relu', input_shape=(5, 321)))  # First RNN layer with 50 units\n",
    "SR_model.add(Dense(1))  # Output layer\n",
    "SR_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7f6707-6d7a-4f38-92cf-989a18a9656d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
    "model.add(LSTM(128, return_sequences=False))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00674d1-f2d3-40e8-852c-3f6528f74260",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model\n",
    "history = model.fit(X_train_pad, y_train, epochs=10, batch_size=32, validation_data=(X_val_pad, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8cdb2c-d7ba-43e7-a690-033fc948656c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_val_pad, y_val)\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = model.predict(X_val_pad)\n",
    "y_pred = (y_pred > 0.5).astype(\"int32\")\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e537f2-9d5d-4c09-a550-42449ee9260c",
   "metadata": {},
   "source": [
    "###False (Class 0):\n",
    "\n",
    "Precision: 1.00\n",
    "All predictions made for the \"False\" class are correct. However, given the other metrics, this likely means very few predictions were made as \"False.\"\n",
    "Recall: 0.02\n",
    "The model only correctly identified 2% of the actual \"False\" instances. This indicates that the model is largely missing instances of this class.\n",
    "F1-Score: 0.04\n",
    "The low F1-score reflects the imbalance between the high precision and very low recall.\n",
    "Support: 55\n",
    "There are 55 instances of the \"False\" class in the dataset.\n",
    "\n",
    "\n",
    "###True (Class 1):\n",
    "Precision: 0.82\n",
    "82% of the predictions for the \"True\" class are correct.\n",
    "Recall: 1.00\n",
    "The model correctly identifies 100% of the \"True\" instances. However, this could mean that the model is over-predicting this class.\n",
    "F1-Score: 0.90\n",
    "The F1-score is high due to the perfect recall and reasonably good precision.\n",
    "Support: 245\n",
    "There are 245 instances of the \"True\" class in the dataset.\n",
    "Accuracy: 0.82\n",
    "\n",
    "The model correctly predicted 82% of the total instances in the dataset.\n",
    "\n",
    "###Macro Average:\n",
    "Precision: 0.91\n",
    "Average precision across both classes.\n",
    "Recall: 0.51\n",
    "Average recall across both classes. The low value here reflects the poor performance on the \"False\" class.\n",
    "F1-Score: 0.47\n",
    "Average F1-score across both classes, showing an imbalance due to the very low performance on the \"False\" class.\n",
    "\n",
    "###Weighted Average:\n",
    "Precision: 0.85\n",
    "Recall: 0.82\n",
    "F1-Score: 0.74\n",
    "These metrics are weighted by the support (number of instances) for each class, showing a more balanced view but still reflecting the issues with class imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe4bb50-877b-4d18-bd25-d5cc619a7aeb",
   "metadata": {},
   "source": [
    "Interpretation of the Results\n",
    "Class Imbalance: The model seems to have a significant class imbalance issue, where it overwhelmingly predicts the \"True\" class (Class 1) and fails to correctly identify the \"False\" class (Class 0).\n",
    "\n",
    "High Precision for False but Low Recall: The model is very conservative when predicting the \"False\" class (high precision), but it misses most of the actual \"False\" instances (low recall).\n",
    "\n",
    "Potential Overfitting to True Class: The model might be overfitting to the \"True\" class, leading to high recall and reasonable precision but at the cost of completely neglecting the \"False\" class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdbcd57-7a62-4037-8dae-cbde68cd0385",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Extra Stuff - To review  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf0b6b0-f2da-4cb6-9268-4765031e6e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysentiment2 as ps\n",
    "dc = ps.HIV4() # import Harvard IV-4 dictionary\n",
    "\n",
    "def get_sentiment_score(text):\n",
    "    score = round(dc.get_score(dc.tokenize(text))['Polarity'], 2)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a231e811-77ce-4256-9ca1-e71d5c7294da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the cleaning\n",
    "# CAUTION: depending on your data set size, the processing might take a while \n",
    "import time  # To keep an eye on runtimes\n",
    "start = time.time()\n",
    "imdb_data['review_clean'] = text_cleaning(imdb_data.review)\n",
    "print('Duration: {:.0f} sec'.format(time.time()-start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
